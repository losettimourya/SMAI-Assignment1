{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from the .npy file\n",
    "data = np.load('data.npy', allow_pickle=True)\n",
    "\n",
    "# Extract the labels (assuming labels are in the 4th column)\n",
    "labels = data[:, 3]\n",
    "\n",
    "# Calculate the frequency of each label using NumPy's unique function\n",
    "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Create a bar graph to visualize the label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(unique_labels, label_counts, align='center', alpha=0.7)\n",
    "plt.xlabel('Label Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Label Distribution in the Dataset')\n",
    "plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5, distance_metric='manhattan', encoder_type=None):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.encoder_type = encoder_type\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "    def train_val_split(self,X, y, test_size=0.2, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.permutation(len(X))\n",
    "        split_index = int(len(X) * (1 - test_size))\n",
    "        train_indices, val_indices = indices[:split_index], indices[split_index:]\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_val]\n",
    "            \n",
    "        return x_train, x_val, y_train, y_val\n",
    "\n",
    "    def unshuffled_train_val_split(self,X,y,test_size=0.2):\n",
    "        total_samples = len(X)\n",
    "        split_index = int(total_samples * (1 - test_size))\n",
    "\n",
    "        X_train = X[:split_index]\n",
    "        X_test = X[split_index:]\n",
    "        y_train = y[:split_index]\n",
    "        y_test = y[split_index:]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_test]\n",
    "        return x_train, x_val, y_train, y_test\n",
    "    def fit(self, data):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_vit, y)\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_resnet, y)\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=2)\n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=1)\n",
    "    def cosine_distance(self, x1, x2) -> float:\n",
    "        return 1-np.dot(x2,x1)/(np.linalg.norm(x2,axis=1)*np.linalg.norm(x1))\n",
    "    def calculate_distance(self, x1, x2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            return self.cosine_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "    def fitpt2(self, data, X_test):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_vit\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_resnet\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        \n",
    "    def predict(self):\n",
    "        y_pred = []\n",
    "        for x in self.X_test:\n",
    "            distances = self.calculate_distance(x, self.X_train)\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            k_indices = sorted_indices[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "            pred_label = unique_labels[np.argmax(counts)]\n",
    "            y_pred.append(pred_label)\n",
    "        print(self.encoder_type)\n",
    "        print(\"Accuracy:\" + str(accuracy_score(self.y_test,y_pred)))\n",
    "        print(\"F1 Score:\" + str(f1_score(self.y_test,y_pred,average='macro')))\n",
    "        print(\"Precision score:\" + str(precision_score(self.y_test,y_pred,average='macro',zero_division=0)))\n",
    "        print(\"Recall score:\" + str(recall_score(self.y_test,y_pred,average='macro',zero_division=0)))\n",
    "def train_val_split(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    train_indices, val_indices = indices[:split_index], indices[split_index:]\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "    x_train = [item[0][0] for item in X_train]\n",
    "    x_val = [item[0][0] for item in X_val]\n",
    "        \n",
    "    return x_train, x_val, y_train, y_val\n",
    "data = np.load('data.npy', allow_pickle=True)\n",
    "X_vit = data[:, 2:3]\n",
    "y = data[:, 3] \n",
    "X_train, X_test, y_train, y_test = train_val_split(X_vit, y)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "start_time = time.time()\n",
    "vit_knn = KNNClassifier(k=3, distance_metric='euclidean',encoder_type='VIT')\n",
    "vit_knn.fit(data=data)\n",
    "vit_knn.predict()\n",
    "end_time = time.time()\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "end2_time = time.time()\n",
    "resnet_knn = KNNClassifier(k=5, distance_metric='cosine',encoder_type='Resnet')\n",
    "resnet_knn.fit(data=data)\n",
    "resnet_knn.predict()\n",
    "execution_time = end_time - start_time\n",
    "execution_time2 = end2_time - end_time\n",
    "print(f\"Execution time1: {execution_time} seconds\")\n",
    "print(f\"Execution time2: {execution_time2} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1.1 & 2.4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5, distance_metric='manhattan', encoder_type=None):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.encoder_type = encoder_type\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "    def train_val_split(self,X, y, test_size=0.2, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.permutation(len(X))\n",
    "        split_index = int(len(X) * (1 - test_size))\n",
    "        train_indices, val_indices = indices[:split_index], indices[split_index:]\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_val]\n",
    "            \n",
    "        return x_train, x_val, y_train, y_val\n",
    "\n",
    "    def unshuffled_train_val_split(self,X,y,test_size=0.2):\n",
    "        total_samples = len(X)\n",
    "        split_index = int(total_samples * (1 - test_size))\n",
    "\n",
    "        X_train = X[:split_index]\n",
    "        X_test = X[split_index:]\n",
    "        y_train = y[:split_index]\n",
    "        y_test = y[split_index:]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_test]\n",
    "        return x_train, x_val, y_train, y_test\n",
    "    def fit(self, data):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_vit, y)\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_resnet, y)\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        # return np.sqrt(np.sum((x1 - x2) ** 2),axis=1)\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=2)\n",
    "\n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=1)\n",
    "\n",
    "    def cosine_distance(self, x1, x2):\n",
    "        return 1-np.dot(x2,x1)/(np.linalg.norm(x2,axis=1)*np.linalg.norm(x1))\n",
    "\n",
    "    def calculate_distance(self, x1, x2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            return self.cosine_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "    def fitpt2(self, data, X_test):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_vit\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_resnet\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        \n",
    "    def predict(self):\n",
    "        y_pred = []\n",
    "        for x in self.X_test:\n",
    "            distances = self.calculate_distance(x, self.X_train)\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            k_indices = sorted_indices[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "            pred_label = unique_labels[np.argmax(counts)]\n",
    "            y_pred.append(pred_label)\n",
    "        return accuracy_score(self.y_test,y_pred)\n",
    "    \n",
    "data = np.load('data.npy', allow_pickle=True)\n",
    "k_value = []\n",
    "accuracy = []\n",
    "answer = []\n",
    "for i in ['VIT', 'Resnet']:\n",
    "    for j in ['manhattan', 'euclidean', 'cosine']:\n",
    "        for kk in range(1,10,2):\n",
    "            knn = KNNClassifier(k=kk,distance_metric=j,encoder_type=i)\n",
    "            knn.fit(data=data)\n",
    "            ans = knn.predict()\n",
    "            answer.append([ans,kk,j,i])\n",
    "            k_value.append(i)\n",
    "            accuracy.append(ans)\n",
    "answer.sort(reverse=True)\n",
    "print(\"The best triplet is: \")\n",
    "print(\"K = \" + str(answer[0][1]))\n",
    "print(\"Distance metric = \" + str(answer[0][2]))\n",
    "print(\"Encoder type: \" + str(answer[0][3]))\n",
    "j=0\n",
    "print()\n",
    "print(\"The top 20 triplets are: \")\n",
    "for i in answer:\n",
    "    print(i)\n",
    "    j += 1\n",
    "    if(j == 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5, distance_metric='manhattan', encoder_type=None):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.encoder_type = encoder_type\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "    def train_val_split(self,X, y, test_size=0.2, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.permutation(len(X))\n",
    "        split_index = int(len(X) * (1 - test_size))\n",
    "        train_indices, val_indices = indices[:split_index], indices[split_index:]\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_val]\n",
    "            \n",
    "        return x_train, x_val, y_train, y_val\n",
    "\n",
    "    def unshuffled_train_val_split(self,X,y,test_size=0.2):\n",
    "        total_samples = len(X)\n",
    "        split_index = int(total_samples * (1 - test_size))\n",
    "\n",
    "        X_train = X[:split_index]\n",
    "        X_test = X[split_index:]\n",
    "        y_train = y[:split_index]\n",
    "        y_test = y[split_index:]\n",
    "        x_train = [item[0][0] for item in X_train]\n",
    "        x_val = [item[0][0] for item in X_test]\n",
    "        return x_train, x_val, y_train, y_test\n",
    "    def fit(self, data):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_vit, y)\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.train_val_split(X_resnet, y)\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        # return np.sqrt(np.sum((x1 - x2) ** 2),axis=1)\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=2)\n",
    "\n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        return np.linalg.norm(x1-x2,axis=1,ord=1)\n",
    "\n",
    "    def cosine_distance(self, x1, x2):\n",
    "        return 1-np.dot(x2,x1)/(np.linalg.norm(x2,axis=1)*np.linalg.norm(x1))\n",
    "\n",
    "    def calculate_distance(self, x1, x2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            return self.cosine_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "    def fitpt2(self, data, X_test):\n",
    "        if self.encoder_type == 'VIT':\n",
    "            X_vit = data[:, 2:3]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_vit\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        elif self.encoder_type == 'Resnet':\n",
    "            X_resnet = data[:, 1:2]\n",
    "            y = data[:, 3] \n",
    "            self.X_train = X_resnet\n",
    "            self.y_train = y\n",
    "            self.X_test = X_test\n",
    "        \n",
    "    def predict(self):\n",
    "        y_pred = []\n",
    "        #print(len(X_test[0][0]))\n",
    "        #print(X_test)\n",
    "        for x in self.X_test:\n",
    "            #print(x)\n",
    "            #distances = [self.calculate_distance(x[0][0], x_train[0][0]) for x_train in self.X_train]\n",
    "            #self.X_train = self.X_train[0]\n",
    "            # print(len(self.X_train))\n",
    "            # print(len(x[0][0]))\n",
    "            distances = self.calculate_distance(x, self.X_train)\n",
    "            # distances = np.array([self.calculate_distance(x[0][0], x_train[0][0]) for x_train in self.X_train])\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            k_indices = sorted_indices[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
    "            pred_label = unique_labels[np.argmax(counts)]\n",
    "            y_pred.append(pred_label)\n",
    "        return accuracy_score(self.y_test,y_pred)\n",
    "        #return np.array(y_pred)\n",
    "\n",
    "# Load the dataset from data.npy\n",
    "data = np.load('data.npy', allow_pickle=True)\n",
    "# X_resnet = data[:, 1:2] \n",
    "# X_vit = data[:, 2:3]\n",
    "# y = data[:, 3] \n",
    "    \n",
    "\n",
    "# X_resnet_train, X_resnet_test, y_resnet_train, y_resnet_test = train_val_split(X_resnet, y)\n",
    "# X_vit_train, X_vit_test, y_vit_train, y_vit_test = train_val_split(X_vit, y)\n",
    "k_value = []\n",
    "accuracy = []\n",
    "for i in range(1,10,2):\n",
    "    knn = KNNClassifier(k=i,distance_metric='euclidean',encoder_type='VIT')\n",
    "    knn.fit(data=data)\n",
    "    ans = knn.predict()\n",
    "    k_value.append(i)\n",
    "    accuracy.append(ans)\n",
    "\n",
    "print(k_value)\n",
    "print(accuracy)\n",
    "plt.plot(k_value, accuracy, marker='o', linestyle='-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K vs Accuracy graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 (MultiOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class MultiOutputDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, max_features='auto', criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y = mlb.fit_transform(y.str.split(' '))\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv')  # Replace with your CSV file path\n",
    "print(\"Original DataFrame:\")\n",
    "print(data.head())\n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "print(\"Encoded DataFrame:\")\n",
    "print(data_encoded.head())\n",
    "X = data_encoded.drop('labels', axis=1)  # Features\n",
    "y = data_encoded['labels']  # Target variable\n",
    "print(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_train)\n",
    "mlb = MultiLabelBinarizer()\n",
    "# y_train = y_train.str.get_dummies(sep=' ')\n",
    "# y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val.str.split(' '))\n",
    "# y_val = y_val.str.get_dummies(sep=' ')\n",
    "# print(y_train)\n",
    "print(y_val)\n",
    "clf = MultiOutputDecisionTreeClassifier(max_depth=5, max_features = 5,criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "print(val_predictions)\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "micro_f1 = f1_score(y_val, val_predictions, average='micro')\n",
    "macro_f1 = f1_score(y_val, val_predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(y_val.argmax(axis=1), val_predictions.argmax(axis=1))\n",
    "precision = precision_score(y_val, val_predictions, average='micro')\n",
    "recall = recall_score(y_val, val_predictions, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'F1 (Micro): {micro_f1:.2f}')\n",
    "print(f'F1 (Macro): {macro_f1:.2f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print(f'Precision (Micro): {precision:.2f}')\n",
    "print(f'Recall (Micro): {recall:.2f}')\n",
    "# val_predictions = mlb.inverse_transform(val_predictions)\n",
    "# y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices for each label\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Compute confusion matrix for each label\n",
    "# for label_idx, label in enumerate(mlb.classes_):\n",
    "#     y_val_label = [1 if label in labels else 0 for labels in y_val]\n",
    "#     val_predictions_label = [1 if label in labels else 0 for labels in val_predictions]\n",
    "#     cm = confusion_matrix(y_val_label, val_predictions_label)\n",
    "#     confusion_matrices[label] = cm\n",
    "\n",
    "# # Print confusion matrices for each label\n",
    "# for label, cm in confusion_matrices.items():\n",
    "#     print(f'Confusion Matrix for Label \"{label}\":')\n",
    "#     print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 (Powerset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class PowersetDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, max_features='auto', criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(max_depth=max_depth,max_features=max_features,criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv')  # Replace with your CSV file path\n",
    "# print(\"Original DataFrame:\")\n",
    "# print(data.head())\n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "# print(\"Encoded DataFrame:\")\n",
    "# print(data_encoded.head())\n",
    "X = data_encoded.drop('labels', axis=1)  # Features\n",
    "y = data_encoded['labels']  # Target variable\n",
    "# print(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(y_train)\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    unique_combos = set()\n",
    "    for r in range(0,len(s) + 1):\n",
    "        for combo in combinations(sorted(s), r):\n",
    "            # return tuple(combo)\n",
    "            unique_combos.add(tuple(combo))\n",
    "    return list(unique_combos)\n",
    "label_combinations = y_train.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "label_combinations_val = y_val.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "print(label_combinations)\n",
    "unique_labels = y.str.split(' ').explode().unique()  \n",
    "all_labels = list(chain.from_iterable(combinations(unique_labels, r) for r in range(len(unique_labels) + 1)))\n",
    "all_label_combinations = all_labels\n",
    "#print(label_combinations_val)\n",
    "# all_label_combinations = list(set(label_combinations.sum() + label_combinations_val.sum()))\n",
    "# print(all_label_combinations)\n",
    "print(label_combinations_val)\n",
    "label_combinations_list = label_combinations.tolist()\n",
    "label_combinations_val_list = label_combinations_val.tolist()\n",
    "print(label_combinations_val_list)\n",
    "#print(label_combinations_list)\n",
    "mlb = MultiLabelBinarizer(classes=all_label_combinations)\n",
    "y_train = mlb.fit_transform(label_combinations_list)\n",
    "y_train_df = pd.DataFrame(y_train, columns=mlb.classes_)\n",
    "y_train = y_train_df.loc[:, ~y_train_df.columns.duplicated()]\n",
    "y_val = mlb.fit_transform(label_combinations_val_list)\n",
    "y_val_df = pd.DataFrame(y_val, columns=mlb.classes_)\n",
    "# # Remove duplicate columns, if any\n",
    "y_val = y_val_df.loc[:, ~y_val_df.columns.duplicated()]\n",
    "# # y_val = mlb.fit_transform(y_val.apply(lambda x: tuple(x.split(' '))))\n",
    "#print(y_train)\n",
    "print(y_val)\n",
    "clf = PowersetDecisionTreeClassifier(max_depth=30, max_features = 11,criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "# print(val_predictions)\n",
    "y_val = y_val.to_numpy()\n",
    "print(np.sum(y_val[0]))\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "micro_f1 = f1_score(y_val, val_predictions, average='micro')\n",
    "macro_f1 = f1_score(y_val, val_predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(y_val.argmax(axis=1), val_predictions.argmax(axis=1))\n",
    "precision = precision_score(y_val, val_predictions, average='micro')\n",
    "recall = recall_score(y_val, val_predictions, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.5f}')\n",
    "print(f'F1 (Micro): {micro_f1:.5f}')\n",
    "print(f'F1 (Macro): {macro_f1:.5f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print(f'Precision (Micro): {precision:.5f}')\n",
    "print(f'Recall (Micro): {recall:.5f}')\n",
    "# val_predictions = mlb.inverse_transform(val_predictions)\n",
    "# y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices for each label\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Compute confusion matrix for each label\n",
    "# for label_idx, label in enumerate(mlb.classes_):\n",
    "#     y_val_label = [1 if label in labels else 0 for labels in y_val]\n",
    "#     val_predictions_label = [1 if label in labels else 0 for labels in val_predictions]\n",
    "#     cm = confusion_matrix(y_val_label, val_predictions_label)\n",
    "#     confusion_matrices[label] = cm\n",
    "\n",
    "# # Print confusion matrices for each label\n",
    "# for label, cm in confusion_matrices.items():\n",
    "#     print(f'Confusion Matrix for Label \"{label}\":')\n",
    "#     print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1.1 (MultiOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class MultiOutputDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, max_features='auto', criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(max_depth=max_depth,max_features=max_features,criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv')  # Replace with your CSV file path\n",
    "print(\"Original DataFrame:\")\n",
    "print(data.head())\n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "print(\"Encoded DataFrame:\")\n",
    "print(data_encoded.head())\n",
    "X = data_encoded.drop('labels', axis=1)  # Features\n",
    "y = data_encoded['labels']  # Target variable\n",
    "print(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_train)\n",
    "mlb = MultiLabelBinarizer()\n",
    "# y_train = y_train.str.get_dummies(sep=' ')\n",
    "y_train = mlb.fit_transform(y_train.str.split(' '))\n",
    "y_val = mlb.fit_transform(y_val.str.split(' '))\n",
    "# y_val = y_val.str.get_dummies(sep=' ')\n",
    "# print(y_train)\n",
    "# print(y_val)\n",
    "for i in [3,5,10,20,30]:\n",
    "    for j in [3,5,7,9,11]:\n",
    "        for k in ['gini','entropy']:            \n",
    "            clf = MultiOutputDecisionTreeClassifier(max_depth=i, max_features = j,criterion=k)\n",
    "            clf.fit(X_train, y_train)\n",
    "            val_predictions = clf.predict(X_val)\n",
    "            # print(val_predictions)\n",
    "            accuracy = accuracy_score(y_val, val_predictions)\n",
    "            micro_f1 = f1_score(y_val, val_predictions, average='micro',zero_division=0)\n",
    "            macro_f1 = f1_score(y_val, val_predictions, average='macro',zero_division=0)\n",
    "            conf_matrix = confusion_matrix(y_val.argmax(axis=1), val_predictions.argmax(axis=1))\n",
    "            precision = precision_score(y_val, val_predictions, average='micro')\n",
    "            recall = recall_score(y_val, val_predictions, average='micro')\n",
    "            print(\"Max Depth: \" + str(i) + \" Max Features: \" + str(j) + \" Criterion: \" + str(k))\n",
    "            print(f'Accuracy: {accuracy:.2f}')\n",
    "            print(f'F1 (Micro): {micro_f1:.2f}')\n",
    "            print(f'F1 (Macro): {macro_f1:.2f}')\n",
    "            print('Confusion Matrix:')\n",
    "            print(conf_matrix)\n",
    "            print(f'Precision (Micro): {precision:.2f}')\n",
    "            print(f'Recall (Micro): {recall:.2f}')\n",
    "# val_predictions = mlb.inverse_transform(val_predictions)\n",
    "# y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices for each label\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Compute confusion matrix for each label\n",
    "# for label_idx, label in enumerate(mlb.classes_):\n",
    "#     y_val_label = [1 if label in labels else 0 for labels in y_val]\n",
    "#     val_predictions_label = [1 if label in labels else 0 for labels in val_predictions]\n",
    "#     cm = confusion_matrix(y_val_label, val_predictions_label)\n",
    "#     confusion_matrices[label] = cm\n",
    "\n",
    "# # Print confusion matrices for each label\n",
    "# for label, cm in confusion_matrices.items():\n",
    "#     print(f'Confusion Matrix for Label \"{label}\":')\n",
    "#     print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1.1 (Powerset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class PowersetDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, max_features='auto', criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(max_depth = max_depth, max_features=max_features,criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv')  # Replace with your CSV file path\n",
    "# print(\"Original DataFrame:\")\n",
    "# print(data.head())\n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "# print(\"Encoded DataFrame:\")\n",
    "# print(data_encoded.head())\n",
    "X = data_encoded.drop('labels', axis=1)  # Features\n",
    "y = data_encoded['labels']  # Target variable\n",
    "# print(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(y_train)\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    unique_combos = set()\n",
    "    for r in range(len(s),len(s) + 1):\n",
    "        for combo in combinations(sorted(s), r):\n",
    "            return tuple(combo)\n",
    "            unique_combos.add(tuple(combo))\n",
    "    return list(unique_combos)\n",
    "label_combinations = y_train.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "label_combinations_val = y_val.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "# print(label_combinations)\n",
    "\n",
    "all_label_combinations = list(set(label_combinations.sum() + label_combinations_val.sum()))\n",
    "unique_labels = y.str.split(' ').explode().unique()  \n",
    "all_label_combinations = list(chain.from_iterable(combinations(unique_labels, r) for r in range(len(unique_labels) + 1)))\n",
    "label_combinations_list = label_combinations.tolist()\n",
    "label_combinations_val_list = label_combinations_val.tolist()\n",
    "#print(label_combinations_list)\n",
    "print(label_combinations_val_list)\n",
    "all_label_combinations = sorted(all_label_combinations, key=lambda x: tuple(sorted(x)))\n",
    "mlb = MultiLabelBinarizer(classes=np.arange(len(all_label_combinations)))\n",
    "mlb.fit(all_label_combinations)\n",
    "def custom_binarize(label_combination):\n",
    "    binarized = np.zeros(len(all_label_combinations))\n",
    "    for idx, label_set in enumerate(all_label_combinations):\n",
    "        if set(label_set) == set(label_combination):\n",
    "            binarized[idx] = 1\n",
    "    return binarized\n",
    "\n",
    "# Binarize y_train and y_val using the custom binarization function\n",
    "print(y_val)\n",
    "y_train = np.array([custom_binarize(labels) for labels in label_combinations_list])\n",
    "y_val = np.array([custom_binarize(labels) for labels in label_combinations_val_list])\n",
    "# y_train = mlb.fit_transform(label_combinations_list)\n",
    "# y_train_df = pd.DataFrame(y_train, columns=mlb.classes_)\n",
    "# y_train = y_train_df.loc[:, ~y_train_df.columns.duplicated()]\n",
    "# y_val = mlb.fit_transform(label_combinations_val_list)\n",
    "# y_val_df = pd.DataFrame(y_val, columns=mlb.classes_)\n",
    "# # # Remove duplicate columns, if any\n",
    "# y_val = y_val_df.loc[:, ~y_val_df.columns.duplicated()]\n",
    "# # y_val = mlb.fit_transform(y_val.apply(lambda x: tuple(x.split(' '))))\n",
    "#print(y_train)\n",
    "# print(y_val)\n",
    "# print(np.sum(y_val[1]))\n",
    "for i in [3,5,10,20,30]:\n",
    "    for j in [3,5,7,9,11]:\n",
    "            for k in ['gini','entropy']:  \n",
    "                clf = PowersetDecisionTreeClassifier(max_depth=i, max_features = j,criterion=k)\n",
    "                clf.fit(X_train, y_train)\n",
    "                val_predictions = clf.predict(X_val)\n",
    "                # print(val_predictions)\n",
    "                # y_val = y_val.to_numpy()\n",
    "                # print(np.sum(y_val[0]))\n",
    "                # print(val_predictions)\n",
    "                accuracy = accuracy_score(y_val, val_predictions)\n",
    "                micro_f1 = f1_score(y_val, val_predictions, average='micro',zero_division=0)\n",
    "                macro_f1 = f1_score(y_val, val_predictions, average='macro',zero_division=0)\n",
    "                conf_matrix = confusion_matrix(y_val.argmax(axis=1), val_predictions.argmax(axis=1))\n",
    "                precision = precision_score(y_val, val_predictions, average='micro',zero_division=0)\n",
    "                recall = recall_score(y_val, val_predictions, average='micro',zero_division=0)\n",
    "                print(\"Max Depth: \" + str(i) + \" Max Features: \" + str(j) + \" Criterion: \" + str(k))\n",
    "                print(f'Accuracy: {accuracy:.5f}')\n",
    "                print(f'F1 (Micro): {micro_f1:.5f}')\n",
    "                print(f'F1 (Macro): {macro_f1:.5f}')\n",
    "                print('Confusion Matrix:')\n",
    "                print(conf_matrix)\n",
    "                print(f'Precision (Micro): {precision:.5f}')\n",
    "                print(f'Recall (Micro): {recall:.5f}')\n",
    "# val_predictions = mlb.inverse_transform(val_predictions)\n",
    "# y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices for each label\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Compute confusion matrix for each label\n",
    "# for label_idx, label in enumerate(mlb.classes_):\n",
    "#     y_val_label = [1 if label in labels else 0 for labels in y_val]\n",
    "#     val_predictions_label = [1 if label in labels else 0 for labels in val_predictions]\n",
    "#     cm = confusion_matrix(y_val_label, val_predictions_label)\n",
    "#     confusion_matrices[label] = cm\n",
    "\n",
    "# # Print confusion matrices for each label\n",
    "# for label, cm in confusion_matrices.items():\n",
    "#     print(f'Confusion Matrix for Label \"{label}\":')\n",
    "#     print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1.2 (MultiOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class MultiOutputDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth, max_features, criterion):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(max_depth=max_depth, max_features = max_features,criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv') \n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "X = data_encoded.drop('labels', axis=1) \n",
    "y = data_encoded['labels'] \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "# test_size = 0.2\n",
    "\n",
    "# # Calculate the number of data points to reserve for testing\n",
    "# num_test_samples = int(len(X) * test_size)\n",
    "\n",
    "# # Create the training and testing sets without shuffling\n",
    "# X_train, X_val = X[:num_test_samples], X[num_test_samples:]\n",
    "# y_train, y_val = y[:num_test_samples], y[num_test_samples:]\n",
    "mlb = MultiLabelBinarizer()\n",
    "# y_train = y_train.str.get_dummies(sep=' ')\n",
    "y_train = mlb.fit_transform(y_train.str.split(' '))\n",
    "y_val = mlb.fit_transform(y_val.str.split(' '))\n",
    "print(y_train[1])\n",
    "ans = []\n",
    "for i in [3,5,10,20,30]:\n",
    "    for j in [3,5,7,9,11]:\n",
    "        for k in ['gini','entropy']:\n",
    "            clf = MultiOutputDecisionTreeClassifier(max_depth=i, max_features = j,criterion=k)\n",
    "            clf.fit(X_train, y_train)\n",
    "            val_predictions = clf.predict(X_val)\n",
    "            # print(y_val)\n",
    "            macro_f1 = f1_score(y_val, val_predictions, average='macro')\n",
    "            ans.append([macro_f1,i,j,k])\n",
    "ans.sort(reverse=True)\n",
    "for i in range(0,3):\n",
    "    print(\"F1 Score: \" + str(ans[i][0]) + \" Max Depth: \" + str(ans[i][1]) + \" Max Features: \" + str(ans[i][2]) + \" Criterion: \" + str(ans[i][3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1.2 (Powerset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "class PowersetDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=5, max_features='auto', criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.classifier = DecisionTreeClassifier(max_depth = max_depth, max_features=max_features,criterion=criterion,random_state=42)\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "data = pd.read_csv('advertisement.csv') \n",
    "data.fillna(method='ffill', inplace=True) \n",
    "categorical_cols = ['gender', 'education', 'married', 'city', 'occupation', 'most bought item']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "X = data_encoded.drop('labels', axis=1)  \n",
    "y = data_encoded['labels']  \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    unique_combos = set()\n",
    "    for r in range(len(s),len(s) + 1):\n",
    "        for combo in combinations(sorted(s), r):\n",
    "            return tuple(combo)\n",
    "            unique_combos.add(tuple(combo))\n",
    "    return list(unique_combos)\n",
    "label_combinations = y_train.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "label_combinations_val = y_val.apply(lambda x: tuple(x.split(' '))).apply(powerset)\n",
    "all_label_combinations = list(set(label_combinations.sum() + label_combinations_val.sum()))\n",
    "unique_labels = y.str.split(' ').explode().unique()  \n",
    "all_label_combinations = list(chain.from_iterable(combinations(unique_labels, r) for r in range(len(unique_labels) + 1)))\n",
    "label_combinations_list = label_combinations.tolist()\n",
    "label_combinations_val_list = label_combinations_val.tolist()\n",
    "all_label_combinations = sorted(all_label_combinations, key=lambda x: tuple(sorted(x)))\n",
    "mlb = MultiLabelBinarizer(classes=np.arange(len(all_label_combinations)))\n",
    "mlb.fit(all_label_combinations)\n",
    "def custom_binarize(label_combination):\n",
    "    binarized = np.zeros(len(all_label_combinations))\n",
    "    for idx, label_set in enumerate(all_label_combinations):\n",
    "        if set(label_set) == set(label_combination):\n",
    "            binarized[idx] = 1\n",
    "    return binarized\n",
    "\n",
    "# Binarize y_train and y_val using the custom binarization function\n",
    "print(label_combinations_list[0])\n",
    "print(label_combinations_val_list[0])\n",
    "y_train = np.array([custom_binarize(labels) for labels in label_combinations_list])\n",
    "y_val = np.array([custom_binarize(labels) for labels in label_combinations_val_list])\n",
    "# print(np.sum(y_train[0])\n",
    "# )\n",
    "# y_train = mlb.fit_transform(label_combinations_list)\n",
    "# y_train_df = pd.DataFrame(y_train, columns=mlb.classes_)\n",
    "# y_train = y_train_df.loc[:, ~y_train_df.columns.duplicated()]\n",
    "# y_val = mlb.fit_transform(label_combinations_val_list)\n",
    "# y_val_df = pd.DataFrame(y_val, columns=mlb.classes_)\n",
    "# # # Remove duplicate columns, if any\n",
    "# y_val = y_val_df.loc[:, ~y_val_df.columns.duplicated()]\n",
    "# # y_val = mlb.fit_transform(y_val.apply(lambda x: tuple(x.split(' '))))\n",
    "#print(y_train)\n",
    "# print(y_val)\n",
    "# print(np.sum(y_val[1]))\n",
    "ans = []\n",
    "for i in [3,5,10,20,30]:\n",
    "    for j in [3,5,7,9,11]:\n",
    "            for k in ['gini','entropy']:  \n",
    "                clf = PowersetDecisionTreeClassifier(max_depth=i, max_features = j,criterion=k)\n",
    "                clf.fit(X_train, y_train)\n",
    "                val_predictions = clf.predict(X_val)\n",
    "                # sum = 0\n",
    "                # for i in range(len(val_predictions)):\n",
    "                #     sum = sum + np.sum(val_predictions[i])\n",
    "                # print(sum)\n",
    "                # y_val = y_val.to_numpy()\n",
    "                # print(np.sum(y_val[0]))\n",
    "                # print(val_predictions)\n",
    "                micro_f1 = f1_score(y_val, val_predictions, average='micro', zero_division=0)\n",
    "                macro_f1 = f1_score(y_val, val_predictions, average='macro',zero_division=0)\n",
    "                ans.append([macro_f1,i,j,k,val_predictions])\n",
    "ans.sort(reverse=True)\n",
    "for i in range(0,3):\n",
    "    print(np.sum(ans[i][4]))\n",
    "    print(\"F1 Score: \" + str(ans[i][0]) + \" Max Depth: \" + str(ans[i][1]) + \" Max Features: \" + str(ans[i][2]) + \" Criterion: \" + str(ans[i][3]))\n",
    "# val_predictions = mlb.inverse_transform(val_predictions)\n",
    "# y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices for each label\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Compute confusion matrix for each label\n",
    "# for label_idx, label in enumerate(mlb.classes_):\n",
    "#     y_val_label = [1 if label in labels else 0 for labels in y_val]\n",
    "#     val_predictions_label = [1 if label in labels else 0 for labels in val_predictions]\n",
    "#     cm = confusion_matrix(y_val_label, val_predictions_label)\n",
    "#     confusion_matrices[label] = cm\n",
    "\n",
    "# # Print confusion matrices for each label\n",
    "# for label, cm in confusion_matrices.items():\n",
    "#     print(f'Confusion Matrix for Label \"{label}\":')\n",
    "#     print(cm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
